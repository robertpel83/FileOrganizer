<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<title>OpenCV: Understanding k-Nearest Neighbour</title>
<link href="../../opencv.ico" rel="shortcut icon" type="image/x-icon" />
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../tutorial-utils.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
//<![CDATA[
MathJax.Hub.Config(
{
  TeX: {
      Macros: {
          matTT: [ "\\[ \\left|\\begin{array}{ccc} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{array}\\right| \\]", 9],
          fork: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ \\end{array} \\right.", 4],
          forkthree: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ \\end{array} \\right.", 6],
          forkfour: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ #7 & \\mbox{#8}\\\\ \\end{array} \\right.", 8],
          vecthree: ["\\begin{bmatrix} #1\\\\ #2\\\\ #3 \\end{bmatrix}", 3],
          vecthreethree: ["\\begin{bmatrix} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{bmatrix}", 9],
          cameramatrix: ["#1 = \\begin{bmatrix} f_x & 0 & c_x\\\\ 0 & f_y & c_y\\\\ 0 & 0 & 1 \\end{bmatrix}", 1],
          distcoeffs: ["(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]]) \\text{ of 4, 5, 8, 12 or 14 elements}"],
          distcoeffsfisheye: ["(k_1, k_2, k_3, k_4)"],
          hdotsfor: ["\\dots", 1],
          mathbbm: ["\\mathbb{#1}", 1],
          bordermatrix: ["\\matrix{#1}", 1]
      }
  }
}
);
//]]>
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<!--#include virtual="/google-search.html"-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../opencv-logo-small.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenCV
   &#160;<span id="projectnumber">4.6.0</span>
   </div>
   <div id="projectbrief">Open Source Computer Vision</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d6/d00/tutorial_py_root.html">OpenCV-Python Tutorials</a></li><li class="navelem"><a class="el" href="../../d6/de2/tutorial_py_table_of_contents_ml.html">Machine Learning</a></li><li class="navelem"><a class="el" href="../../d0/d72/tutorial_py_knn_index.html">K-Nearest Neighbour</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Understanding k-Nearest Neighbour </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Goal </h2>
<p>In this chapter, we will understand the concepts of the k-Nearest Neighbour (kNN) algorithm.</p>
<h2>Theory </h2>
<p>kNN is one of the simplest classification algorithms available for supervised learning. The idea is to search for the closest match(es) of the test data in the feature space. We will look into it with the below image.</p>
<div class="image">
<img src="../../knn_theory.png" alt="knn_theory.png"/>
<div class="caption">
image</div></div>
<p> In the image, there are two families: Blue Squares and Red Triangles. We refer to each family as a <b>Class</b>. Their houses are shown in their town map which we call the <b>Feature Space</b>. You can consider a feature space as a space where all data are projected. For example, consider a 2D coordinate space. Each datum has two features, a x coordinate and a y coordinate. You can represent this datum in your 2D coordinate space, right? Now imagine that there are three features, you will need 3D space. Now consider N features: you need N-dimensional space, right? This N-dimensional space is its feature space. In our image, you can consider it as a 2D case with two features.</p>
<p>Now consider what happens if a new member comes into the town and creates a new home, which is shown as the green circle. He should be added to one of these Blue or Red families (or <em>classes</em>). We call that process, <b>Classification</b>. How exactly should this new member be classified? Since we are dealing with kNN, let us apply the algorithm.</p>
<p>One simple method is to check who is his nearest neighbour. From the image, it is clear that it is a member of the Red Triangle family. So he is classified as a Red Triangle. This method is called simply <b>Nearest Neighbour</b> classification, because classification depends only on the <em>nearest neighbour</em>.</p>
<p>But there is a problem with this approach! Red Triangle may be the nearest neighbour, but what if there are also a lot of Blue Squares nearby? Then Blue Squares have more strength in that locality than Red Triangles, so just checking the nearest one is not sufficient. Instead we may want to check some <b>k</b> nearest families. Then whichever family is the majority amongst them, the new guy should belong to that family. In our image, let's take k=3, i.e. consider the 3 nearest neighbours. The new member has two Red neighbours and one Blue neighbour (there are two Blues equidistant, but since k=3, we can take only one of them), so again he should be added to Red family. But what if we take k=7? Then he has 5 Blue neighbours and 2 Red neighbours and should be added to the Blue family. The result will vary with the selected value of k. Note that if k is not an odd number, we can get a tie, as would happen in the above case with k=4. We would see that our new member has 2 Red and 2 Blue neighbours as his four nearest neighbours and we would need to choose a method for breaking the tie to perform classification. So to reiterate, this method is called <b>k-Nearest Neighbour</b> since classification depends on the <em>k nearest neighbours</em>.</p>
<p>Again, in kNN, it is true we are considering k neighbours, but we are giving equal importance to all, right? Is this justified? For example, take the tied case of k=4. As we can see, the 2 Red neighbours are actually closer to the new member than the other 2 Blue neighbours, so he is more eligible to be added to the Red family. How do we mathematically explain that? We give some weights to each neighbour depending on their distance to the new-comer: those who are nearer to him get higher weights, while those that are farther away get lower weights. Then we add the total weights of each family separately and classify the new-comer as part of whichever family received higher total weights. This is called <b>modified kNN</b> or <b>weighted kNN</b>.</p>
<p>So what are some important things you see here?</p>
<ul>
<li>Because we have to check the distance from the new-comer to all the existing houses to find the nearest neighbour(s), you need to have information about all of the houses in town, right? If there are plenty of houses and families, it takes a lot of memory, and also more time for calculation.</li>
<li>There is almost zero time for any kind of "training" or preparation. Our "learning" involves only memorizing (storing) the data, before testing and classifying.</li>
</ul>
<p>Now let's see this algorithm at work in OpenCV.</p>
<h2>kNN in OpenCV </h2>
<p>We will do a simple example here, with two families (classes), just like above. Then in the next chapter, we will do an even better example.</p>
<p>So here, we label the Red family as <b>Class-0</b> (so denoted by 0) and Blue family as <b>Class-1</b> (denoted by 1). We create 25 neighbours or 25 training data, and label each of them as either part of Class-0 or Class-1. We can do this with the help of a Random Number Generator from NumPy.</p>
<p>Then we can plot it with the help of Matplotlib. Red neighbours are shown as Red Triangles and Blue neighbours are shown as Blue Squares. </p><div class="fragment"><div class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># Feature set containing (x,y) values of 25 known/training data</span></div><div class="line">trainData = np.random.randint(0,100,(25,2)).astype(np.float32)</div><div class="line"></div><div class="line"><span class="comment"># Label each one either Red or Blue with numbers 0 and 1</span></div><div class="line">responses = np.random.randint(0,2,(25,1)).astype(np.float32)</div><div class="line"></div><div class="line"><span class="comment"># Take Red neighbours and plot them</span></div><div class="line">red = trainData[responses.ravel()==0]</div><div class="line">plt.scatter(red[:,0],red[:,1],80,<span class="stringliteral">&#39;r&#39;,&#39;</span>^&#39;)</div><div class="line"></div><div class="line"><span class="comment"># Take Blue neighbours and plot them</span></div><div class="line">blue = trainData[responses.ravel()==1]</div><div class="line">plt.scatter(blue[:,0],blue[:,1],80,<span class="stringliteral">&#39;b&#39;</span>,<span class="stringliteral">&#39;s&#39;</span>)</div><div class="line"></div><div class="line">plt.show()</div></div><!-- fragment --><p> You will get something similar to our first image. Since you are using a random number generator, you will get different data each time you run the code.</p>
<p>Next initiate the kNN algorithm and pass the trainData and responses to train the kNN. (Underneath the hood, it constructs a search tree: see the Additional Resources section below for more information on this.)</p>
<p>Then we will bring one new-comer and classify him as belonging to a family with the help of kNN in OpenCV. Before running kNN, we need to know something about our test data (data of new comers). Our data should be a floating point array with size \(number \; of \; testdata \times number \; of \; features\). Then we find the nearest neighbours of the new-comer. We can specify <em>k</em>: how many neighbours we want. (Here we used 3.) It returns:</p>
<ol type="1">
<li>The label given to the new-comer depending upon the kNN theory we saw earlier. If you want the <em>Nearest Neighbour</em> algorithm, just specify k=1.</li>
<li>The labels of the k-Nearest Neighbours.</li>
<li>The corresponding distances from the new-comer to each nearest neighbour.</li>
</ol>
<p>So let's see how it works. The new-comer is marked in green. </p><div class="fragment"><div class="line">newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)</div><div class="line">plt.scatter(newcomer[:,0],newcomer[:,1],80,<span class="stringliteral">&#39;g&#39;</span>,<span class="stringliteral">&#39;o&#39;</span>)</div><div class="line"></div><div class="line">knn = cv.ml.KNearest_create()</div><div class="line">knn.train(trainData, cv.ml.ROW_SAMPLE, responses)</div><div class="line">ret, results, neighbours ,dist = knn.findNearest(newcomer, 3)</div><div class="line"></div><div class="line"><a class="code" href="../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366">print</a>( <span class="stringliteral">&quot;result:  {}\n&quot;</span>.format(results) )</div><div class="line"><a class="code" href="../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366">print</a>( <span class="stringliteral">&quot;neighbours:  {}\n&quot;</span>.format(neighbours) )</div><div class="line"><a class="code" href="../../df/d57/namespacecv_1_1dnn.html#a43417dcaeb3c1e2a09b9d948e234c366">print</a>( <span class="stringliteral">&quot;distance:  {}\n&quot;</span>.format(dist) )</div><div class="line"></div><div class="line">plt.show()</div></div><!-- fragment --><p> I got the following results: </p><div class="fragment"><div class="line">result:  [[ 1.]]</div><div class="line">neighbours:  [[ 1.  1.  1.]]</div><div class="line">distance:  [[ 53.  58.  61.]]</div></div><!-- fragment --><p> It says that our new-comer's 3 nearest neighbours are all from the Blue family. Therefore, he is labelled as part of the Blue family. It is obvious from the plot below:</p>
<div class="image">
<img src="../../knn_simple.png" alt="knn_simple.png"/>
<div class="caption">
image</div></div>
<p> If you have multiple new-comers (test data), you can just pass them as an array. Corresponding results are also obtained as arrays. </p><div class="fragment"><div class="line"><span class="comment"># 10 new-comers</span></div><div class="line">newcomers = np.random.randint(0,100,(10,2)).astype(np.float32)</div><div class="line">ret, results,neighbours,dist = knn.findNearest(newcomer, 3)</div><div class="line"><span class="comment"># The results also will contain 10 labels.</span></div></div><!-- fragment --> <h2>Additional Resources </h2>
<ol type="1">
<li><a href="https://nptel.ac.in/courses/106/108/106108057/">NPTEL notes on Pattern Recognition, Chapter 11</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">Wikipedia article on Nearest neighbor search</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-d_tree">Wikipedia article on k-d tree</a></li>
</ol>
<h2>Exercises </h2>
<ol type="1">
<li>Try repeating the above with more classes and different choices of k. Does choosing k become harder with more classes in the same 2D feature space? </li>
</ol>
</div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun Jun 5 2022 16:19:55 for OpenCV by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
<script type="text/javascript">
//<![CDATA[
addTutorialsButtons();
//]]>
</script>
</body>
</html>
