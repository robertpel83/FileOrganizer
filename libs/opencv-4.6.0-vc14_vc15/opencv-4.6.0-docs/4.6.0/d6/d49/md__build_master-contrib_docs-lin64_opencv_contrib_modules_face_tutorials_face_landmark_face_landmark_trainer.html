<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<title>OpenCV: face_landmark_trainer</title>
<link href="../../opencv.ico" rel="shortcut icon" type="image/x-icon" />
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../tutorial-utils.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
//<![CDATA[
MathJax.Hub.Config(
{
  TeX: {
      Macros: {
          matTT: [ "\\[ \\left|\\begin{array}{ccc} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{array}\\right| \\]", 9],
          fork: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ \\end{array} \\right.", 4],
          forkthree: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ \\end{array} \\right.", 6],
          forkfour: ["\\left\\{ \\begin{array}{l l} #1 & \\mbox{#2}\\\\ #3 & \\mbox{#4}\\\\ #5 & \\mbox{#6}\\\\ #7 & \\mbox{#8}\\\\ \\end{array} \\right.", 8],
          vecthree: ["\\begin{bmatrix} #1\\\\ #2\\\\ #3 \\end{bmatrix}", 3],
          vecthreethree: ["\\begin{bmatrix} #1 & #2 & #3\\\\ #4 & #5 & #6\\\\ #7 & #8 & #9 \\end{bmatrix}", 9],
          cameramatrix: ["#1 = \\begin{bmatrix} f_x & 0 & c_x\\\\ 0 & f_y & c_y\\\\ 0 & 0 & 1 \\end{bmatrix}", 1],
          distcoeffs: ["(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]]) \\text{ of 4, 5, 8, 12 or 14 elements}"],
          distcoeffsfisheye: ["(k_1, k_2, k_3, k_4)"],
          hdotsfor: ["\\dots", 1],
          mathbbm: ["\\mathbb{#1}", 1],
          bordermatrix: ["\\matrix{#1}", 1]
      }
  }
}
);
//]]>
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<!--#include virtual="/google-search.html"-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../opencv-logo-small.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenCV
   &#160;<span id="projectnumber">4.6.0</span>
   </div>
   <div id="projectbrief">Open Source Computer Vision</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">face_landmark_trainer </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><div class="image">
<img src="../../2.jpg" alt="2.jpg"/>
</div>
<h1><a class="anchor" id="tutorial_face_training_face_landmark_detector"></a>
Training face landmark detector</h1>
<p>This application helps to train your own face landmark detector. You can train your own face landmark detection by just providing the paths for directory containing the images and files containing their corresponding face landmarks. As this landmark detector was originally trained on <a href="http://www.ifp.illinois.edu/~vuongle2/helen/">HELEN dataset</a>, the training follows the format of data provided in HELEN dataset.</p>
<p>The dataset consists of .txt files whose first line contains the image name which then follows the annotations. The format of the file containing annotations should be of following format : </p><blockquote class="doxtable">
<p>/directory/images/abc.jpg 123.45,345.65 321.67,543.89 .... , .... .... , .... </p>
</blockquote>
<p>The above format is similar to HELEN dataset which is used for training the model.</p>
<div class="fragment"><div class="line">// Command to be typed for running the sample</div><div class="line">./sample_train_landmark_detector -annotations=/home/sukhad/Downloads/code/trainset/ -config=config.xml -face_cascade=lbpcascadefrontalface.xml -model=trained_model.dat -width=460 -height=460</div></div><!-- fragment --><h3>Description of command parameters</h3>
<blockquote class="doxtable">
<ul>
<li><b>annotations</b> a : (REQUIRED) Path to annotations txt file [example - /data/annotations.txt]</li>
<li><b>config</b> c : (REQUIRED) Path to configuration xml file containing parameters for training.[ example - /data/config.xml]</li>
<li><b>model</b> m : (REQUIRED) Path to configuration xml file containing parameters for training.[ example - /data/model.dat]</li>
<li><b>width</b> w : (OPTIONAL) The width which you want all images to get to scale the annotations. Large images are slow to process [default = 460]</li>
<li><b>height</b> h : (OPTIONAL) The height which you want all images to get to scale the annotations. Large images are slow to process [default = 460]</li>
<li><b>face_cascade</b> f (REQUIRED) Path to the face cascade xml file which you want to use as a detector. </li>
</ul>
</blockquote>
<h3>Description of training parameters</h3>
<p>The configuration file described above which is used while training contains the training parameters which are required for training.</p>
<p><b>The description of parameters is as follows :</b></p>
<ol type="1">
<li><b>Cascade depth :</b> This stores the depth of cascade of regressors used for training.</li>
<li><b>Tree depth :</b> This stores the depth of trees created as weak learners during gradient boosting.</li>
<li><b>Number of trees per cascade level :</b> This stores number of trees required per cascade level.</li>
<li><b>Learning rate :</b> This stores the learning rate for gradient boosting.This is required to prevent overfitting using shrinkage.</li>
<li><b>Oversampling amount :</b> This stores the oversampling amount for the samples.</li>
<li><b>Number of test coordinates :</b> This stores number of test coordinates to be generated as samples to decide for making the split.</li>
<li><b>Lambda :</b> This stores the value used for calculating the probabilty which helps to select closer pixels for making the split.</li>
<li><b>Number of test splits :</b> This stores the number of test splits to be generated before making the best split.</li>
</ol>
<p>To get more detailed description about the training parameters you can refer to the <a href="https://pdfs.semanticscholar.org/d78b/6a5b0dcaa81b1faea5fb0000045a62513567.pdf">Research paper</a>.</p>
<h3>Understanding code</h3>
<div class="image">
<img src="../../3.jpg" alt="3.jpg"/>
</div>
<p>Jumping directly to the code :</p>
<div class="fragment"><div class="line">CascadeClassifier face_cascade;</div><div class="line">bool myDetector( InputArray image, OutputArray ROIs );</div><div class="line"></div><div class="line">bool myDetector( InputArray image, OutputArray ROIs ){</div><div class="line">    Mat gray;</div><div class="line">    std::vector&lt;Rect&gt; faces;</div><div class="line">    if(image.channels()&gt;1){</div><div class="line">        cvtColor(image.getMat(),gray,COLOR_BGR2GRAY);</div><div class="line">    }</div><div class="line">    else{</div><div class="line">        gray = image.getMat().clone();</div><div class="line">    }</div><div class="line">    equalizeHist( gray, gray );</div><div class="line">    face_cascade.detectMultiScale( gray, faces, 1.1, 3,0, Size(30, 30) );</div><div class="line">    Mat(faces).copyTo(ROIs);</div><div class="line">    return true;</div><div class="line">}</div></div><!-- fragment --><p> The facemark API provides the functionality to the user to use their own face detector to be used in training.The above code creartes a sample face detector. The above function would be passed to a function pointer in the facemark API.</p>
<div class="fragment"><div class="line">vector&lt;String&gt; filenames;</div><div class="line">glob(directory,filenames);</div></div><!-- fragment --><p> The above code creates a vector filenames for storing the names of the .txt files. It gets the filenames of the files in the directory.</p>
<div class="fragment"><div class="line">Mat img = imread(image);</div><div class="line">face_cascade.load(cascade_name);</div><div class="line">FacemarkKazemi::Params params;</div><div class="line">params.configfile = configfile_name;</div><div class="line">Ptr&lt;Facemark&gt; facemark = FacemarkKazemi::create(params);</div><div class="line">facemark-&gt;setFaceDetector(myDetector);</div></div><!-- fragment --><p> The above code creates a pointer of the face landmark detection class. The face detector created above has to be passed as function pointer to the facemark pointer created for detecting faces while training the model.</p>
<div class="fragment"><div class="line">vector&lt;String&gt; imagenames;</div><div class="line">vector&lt; vector&lt;Point2f&gt; &gt; trainlandmarks,Trainlandmarks;</div><div class="line">vector&lt;Mat&gt; trainimages;</div><div class="line">loadTrainingData(filenames,trainlandmarks,imagenames);</div><div class="line">for(unsigned long i=0;i&lt;300;i++){</div><div class="line">string imgname = imagenames[i].substr(0, imagenames[i].size()-1);</div><div class="line">string img = directory + string(imgname) + &quot;.jpg&quot;;</div><div class="line">Mat src = imread(img);</div><div class="line">if(src.empty()){</div><div class="line">    cerr&lt;&lt;string(&quot;Image &quot;+img+&quot; not found\n.&quot;)&lt;&lt;endl;</div><div class="line">    continue;</div><div class="line">}</div><div class="line">trainimages.push_back(src);</div><div class="line">Trainlandmarks.push_back(trainlandmarks[i]);</div><div class="line">}</div></div><!-- fragment --><p> The above code creates std::vectors to store the images and their corresponding landmarks. The above code calls a function loadTrainingData to load the landmarks and the images into their respective vectors.</p>
<p>If the dataset you downloaded is of the following format : </p><div class="fragment"><div class="line">version: 1</div><div class="line">n_points:  68</div><div class="line">{</div><div class="line"> 115.167660 220.807529</div><div class="line"> 116.164839 245.721357</div><div class="line"> 120.208690 270.389841</div><div class="line">  ...</div><div class="line">}</div><div class="line">This is the example of the dataset available at https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/</div></div><!-- fragment --><p>Then skip the above code for loading training data and use the following code. This sample is provided as sampleTrainLandmarkDetector2.cpp in the face module in opencv contrib.</p>
<div class="fragment"><div class="line">std::vector&lt;String&gt; images;</div><div class="line">std::vector&lt;std::vector&lt;Point2f&gt; &gt; facePoints;</div><div class="line">loadTrainingData(imagesList, annotations, images, facePoints, 0.0);</div></div><!-- fragment --><p>In the above code imagelist and annotations are the file of following format : </p><div class="fragment"><div class="line">example of contents for images.txt:</div><div class="line">../trainset/image_0001.png</div><div class="line">../trainset/image_0002.png</div><div class="line">example of contents for annotation.txt:</div><div class="line">../trainset/image_0001.pts</div><div class="line">../trainset/image_0002.pts</div></div><!-- fragment --><p>These symbolize the names of images and their corresponding annotations.</p>
<p>The above code scales images and landmarks as training on images of smaller size takes less time. This is because processing larger images requires more time. After scaling data it calculates mean shape of the data which is used as initial shape while training.</p>
<p>Finally call the following function to perform training :</p>
<div class="fragment"><div class="line">facemark-&gt;training(Trainimages,Trainlandmarks,configfile_name,scale,modelfile_name);</div></div><!-- fragment --><p> In the above function scale is passed to scale all images and the corresponding landmarks so that the size of all images can be reduced as it takes greater time to process large images. This call to the train function trains the model and stores the trained model file with the given filename specified.As the training starts successfully you will see something like this : </p><div class="image">
<img src="../../train1.png" alt="train1.png"/>
</div>
<p><b>The error rate on trained images depends on the number of images used for training used as follows :</b></p>
<div class="image">
<img src="../../train.png" alt="train.png"/>
</div>
<p><b>The error rate on test images depends on the number of images used for training used as follows :</b></p>
<div class="image">
<img src="../../test.png" alt="test.png"/>
</div>
 </div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sun Jun 5 2022 16:19:56 for OpenCV by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
<script type="text/javascript">
//<![CDATA[
addTutorialsButtons();
//]]>
</script>
</body>
</html>
